# Similarity Processing Pipeline

Single pipeline that combines preprocessing and embedding for similarity results from the Ranker output.

## Overview

This pipeline processes the precise similarity results generated by the Ranker and creates embeddings for ChromaDB storage in one go:

1. **Preprocessing**: Cleans and normalizes text from similarity results
2. **Embedding Generation**: Creates both Qwen and Paraphrase embeddings
3. **ChromaDB Storage**: Stores embeddings in separate collections by type

## Input Files

The pipeline expects these files in `outputs/precise_similarity/`:
- `TNredacted_precise_attribute_similarities.json` → 5 datasets (per source PDF)
- `WAredacted_precise_attribute_similarities.json` → 5 datasets (per source PDF)  
- `TNstandard_precise_attribute_similarities.json` → 1 dataset (global)
- `WAstandard_precise_attribute_similarities.json` → 1 dataset (global)

## Output

### Processed Datasets
- **Location**: `outputs/precise_similarity/processed_datasets/`
- **Format**: JSON arrays with preprocessed content and metadata
- **Files**: ~12 dataset files (5 TN + 5 WA redacted + 2 standard)

### ChromaDB Collections
- **`similarityTemplate_qwen`** - Standard templates with Qwen embeddings
- **`similarityTemplate_para`** - Standard templates with Paraphrase embeddings  
- **`SimilarityRedacted_qwen`** - Redacted contracts with Qwen embeddings
- **`SimilarityRedacted_para`** - Redacted contracts with Paraphrase embeddings

## Features

### Text Preprocessing
- Unicode normalization (NFKC)
- Lowercase conversion
- Markdown artifact removal
- Stopword removal (comprehensive list)
- Number masking (digits → "num")
- Token count calculation (tiktoken preferred)
- Redaction pattern detection
- SHA256 content hashing

### Dual Embeddings
- **Qwen**: Via Ollama API (`qwen3-embedding:0.6b`)
- **Paraphrase**: Via SentenceTransformers (`paraphrase-MiniLM-L6-v2`)
- L2 normalization applied to both

### Metadata Enrichment
Each processed row includes:
- Original similarity scores (RRF, BM25, dense)
- Preprocessing metadata and version
- Token counts and content hashes
- Redaction detection results
- Source attribution (file, page, section, chunk)

## Usage

### Prerequisites
```bash
# Install dependencies
pip install -r PreprocessingSimilarity/requirements.txt

# Ensure Ollama is running with Qwen model
ollama pull qwen3-embedding:0.6b
```

### Run Pipeline
```bash
# Basic usage (from anywhere)
python PreprocessingSimilarity/similarity_pipeline.py

# With custom options
python PreprocessingSimilarity/similarity_pipeline.py \
  --input-dir /path/to/outputs/precise_similarity \
  --output-dir /path/to/outputs/precise_similarity/processed_datasets \
  --db-path /path/to/chroma_db_qwen \
  --ollama-url http://localhost:11434 \
  --batch-size 32
```

### Configuration Options
- `--input-dir`: Directory containing similarity JSON files
- `--output-dir`: Directory for processed datasets  
- `--db-path`: ChromaDB database path
- `--ollama-url`: Ollama server URL
- `--qwen-model`: Qwen embedding model name
- `--paraphrase-model`: SentenceTransformers model name
- `--batch-size`: Batch size for paraphrase embeddings

## Pipeline Steps

### Step 1: Preprocessing
- Loads similarity JSON files from Ranker output
- Applies text normalization and cleaning
- Splits redacted files by source PDF (per-source datasets)
- Creates single datasets for standard templates
- Enforces per-attribute limits (varies by attribute number)
- Saves processed datasets as JSON files

### Step 2: Embedding Generation
- Initializes Qwen (Ollama) and Paraphrase (SentenceTransformers) models
- Processes each dataset file
- Generates embeddings in batches (paraphrase) and sequentially (Qwen)
- Creates separate ChromaDB collections for each embedding type
- Stores rich metadata with each embedding

## Performance

- **Processing Time**: ~5-15 minutes depending on dataset size
- **Memory Usage**: ~2-4GB peak (with batching)
- **Output Size**: ~12 JSON files + ChromaDB collections
- **Embedding Dimensions**: Qwen (1024d), Paraphrase (384d)

## Error Handling

- Validates input file existence before processing
- Graceful fallbacks for optional dependencies (ftfy, tiktoken)
- Skips empty embeddings with logging
- Continues processing if individual embeddings fail
- Comprehensive logging throughout pipeline

## Integration

This pipeline integrates with:
- **Ranker**: Consumes precise similarity results
- **StandardClassification**: Uses processed datasets for classification
- **Chatbot**: Queries ChromaDB collections for contract analysis

## Files Structure

```
PreprocessingSimilarity/
├── similarity_pipeline.py     # Main pipeline script
├── requirements.txt          # Dependencies
├── README.md                # This file
├── preprocess_precise_full_content.py  # Legacy preprocessing (standalone)
└── paraphrase_ollama_embedding.py     # Legacy embedding (standalone)
```

## Notes

- The pipeline replaces the need to run `preprocess_precise_full_content.py` and `paraphrase_ollama_embedding.py` separately
- Uses absolute paths for cross-directory execution
- Maintains compatibility with existing ChromaDB collection names
- Preserves all original metadata from similarity results
- Optimized for memory efficiency with batched processing
