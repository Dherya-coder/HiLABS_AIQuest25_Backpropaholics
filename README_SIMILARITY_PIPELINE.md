# Attribute Similarity Pipeline

This pipeline uses **RRF (Reciprocal Rank Fusion)** to find the most similar contract clauses for each attribute across all contract collections.

## ðŸŽ¯ **What It Does**

For each attribute in the `attributes_simple` collection, finds:
- **Top 10 matches** from `TNstandard` collection
- **Top 10 matches** from `WAstandard` collection  
- **Top 20 matches** from `TNredacted` collection
- **Top 20 matches** from `WAredacted` collection

Uses the proven `rrf_attribute_matcher.py` which combines:
- **Dense similarity** (cosine similarity with embeddings)
- **BM25 sparse retrieval** (keyword matching)
- **RRF fusion** for optimal ranking

## ðŸ“ **Files Structure**

```
â”œâ”€â”€ attribute_similarity_pipeline.py    # Main pipeline
â”œâ”€â”€ run_similarity_analysis.py         # Simple runner script
â”œâ”€â”€ view_similarity_results.py         # Results viewer
â”œâ”€â”€ Ranker/
â”‚   â””â”€â”€ rrf_attribute_matcher.py       # Core RRF matching logic
â””â”€â”€ outputs/similarity/                # Results directory
    â”œâ”€â”€ TNstandard_attribute_similarities.json
    â”œâ”€â”€ WAstandard_attribute_similarities.json
    â”œâ”€â”€ TNredacted_attribute_similarities.json
    â”œâ”€â”€ WAredacted_attribute_similarities.json
    â””â”€â”€ similarity_pipeline_summary.json
```

## ðŸš€ **How to Run**

### **Option 1: Simple One-Command**
```bash
python3 run_similarity_analysis.py
```

### **Option 2: Run Pipeline Directly**
```bash
python3 attribute_similarity_pipeline.py
```

### **Option 3: Step by Step**
```bash
# 1. Check collections
python3 -c "
import chromadb
client = chromadb.PersistentClient(path='chroma_db_qwen')
for name in ['attributes_simple', 'TNstandard', 'WAstandard', 'TNredacted', 'WAredacted']:
    try:
        collection = client.get_collection(name)
        print(f'{name}: {collection.count()} documents')
    except:
        print(f'{name}: Not found')
"

# 2. Run pipeline
python3 attribute_similarity_pipeline.py

# 3. View results
python3 view_similarity_results.py --interactive
```

## ðŸ“Š **Expected Output**

### **Console Output:**
```
ðŸš€ STARTING ATTRIBUTE SIMILARITY PIPELINE
==============================================================
ðŸ” Verifying collections availability...
âœ… attributes_simple: 45 attributes
âœ… TNstandard: 156 documents
âœ… WAstandard: 142 documents  
âœ… TNredacted: 2404 documents
âœ… WAredacted: 1876 documents
ðŸ“Š Will process 4 collections

==================== TNSTANDARD ====================
ðŸ”„ Processing TNstandard (TNstandard)
   Target: Top 10 matches per attribute
âœ… TNstandard: 45 attributes, 450 total matches

==================== WASTANDARD ====================
ðŸ”„ Processing WAstandard (WAstandard)
   Target: Top 10 matches per attribute
âœ… WAstandard: 45 attributes, 450 total matches

ðŸŽ‰ ALL COLLECTIONS PROCESSED SUCCESSFULLY!
ðŸ“ Results saved in: outputs/similarity
```

### **Generated Files:**
- `TNstandard_attribute_similarities.json` (~2-5 MB)
- `WAstandard_attribute_similarities.json` (~2-5 MB)
- `TNredacted_attribute_similarities.json` (~8-15 MB)
- `WAredacted_attribute_similarities.json` (~8-15 MB)
- `similarity_pipeline_summary.json` (~50 KB)

## ðŸ” **Viewing Results**

### **Quick Overview:**
```bash
python3 view_similarity_results.py
```

### **Interactive Exploration:**
```bash
python3 view_similarity_results.py --interactive
```

**Interactive Commands:**
- `overview` - Show results summary
- `collections` - List available collections
- `collection TNredacted` - Show TNredacted details
- `attributes` - List all attributes
- `attributes TNredacted` - List attributes in TNredacted
- `matches TNredacted Provider Network Requirements` - Show matches for specific attribute

### **Specific Collection:**
```bash
python3 view_similarity_results.py --collection TNredacted
```

### **Specific Attribute Matches:**
```bash
python3 view_similarity_results.py --collection TNredacted --attribute "Provider Network Requirements"
```

## ðŸ“‹ **Result Structure**

Each result file contains:

```json
{
  "collection_info": {
    "collection_name": "TNredacted",
    "description": "TN Redacted Contracts",
    "top_k": 20,
    "processed_at": "2025-09-29T01:45:00"
  },
  "summary": {
    "method": "RRF (Dense + BM25)",
    "total_attributes": 45,
    "total_contracts": 2404,
    "rrf_k": 60
  },
  "matches": {
    "Provider Network Requirements": {
      "matches": [
        {
          "rank": 1,
          "rrf_score": 0.0234,
          "page": 15,
          "section": "clause:network_participation",
          "chunk_id": "TN_Contract1_p15_c3",
          "content_preview": "Provider agrees to participate in...",
          "score_breakdown": {
            "dense_similarity": 0.8456,
            "bm25_score": 12.34,
            "dense_rank": 2,
            "bm25_rank": 5,
            "rrf_contribution_dense": 0.0161,
            "rrf_contribution_bm25": 0.0154
          }
        }
      ]
    }
  }
}
```

## âš¡ **Performance Notes**

- **Processing time**: ~2-5 minutes per collection
- **Memory usage**: ~2-4 GB peak
- **Output size**: ~20-40 MB total
- **RRF scoring**: Combines dense + sparse for optimal results

## ðŸŽ¯ **Use Cases**

1. **Contract Analysis**: Find similar clauses across different contract types
2. **Compliance Checking**: Identify how attributes are handled in different contracts
3. **Template Creation**: Extract best practices from similar clauses
4. **Legal Research**: Discover patterns in contract language
5. **Quality Assurance**: Verify consistency across contract collections

## ðŸ”§ **Troubleshooting**

### **Collection Not Found:**
```bash
# Check available collections
python3 -c "
import chromadb
client = chromadb.PersistentClient(path='chroma_db_qwen')
print([c.name for c in client.list_collections()])
"
```

### **Memory Issues:**
- Reduce batch size in `rrf_attribute_matcher.py`
- Process collections individually
- Use smaller `top_k` values

### **No Results:**
- Check if embeddings exist in collections
- Verify attribute collection has content
- Check ChromaDB path is correct

## ðŸ“ˆ **Expected Results Quality**

- **High RRF scores (>0.02)**: Very relevant matches
- **Medium RRF scores (0.01-0.02)**: Moderately relevant  
- **Low RRF scores (<0.01)**: Potentially relevant but lower confidence

The RRF method typically provides better results than pure similarity or pure keyword matching alone.
