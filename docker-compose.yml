version: '3.8'

services:
  # Ollama service for LLM embeddings and chat
  ollama:
    image: ollama/ollama:latest
    container_name: hilabs-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - hilabs-network

  # ChromaDB service for vector storage
  chromadb:
    image: chromadb/chroma:latest
    container_name: hilabs-chromadb
    ports:
      - "8001:8000"
    volumes:
      - chromadb_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      - hilabs-network

  # Main HiLABS preprocessing pipeline
  hilabs-pipeline:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: hilabs-pipeline
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data:ro
      - ./outputs:/app/outputs
      - pipeline_logs:/app/logs
    environment:
      - PYTHONPATH=/app
      - OLLAMA_URL=http://ollama:11434
      - CHROMADB_URL=http://chromadb:8000
      - LOG_LEVEL=INFO
    depends_on:
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    networks:
      - hilabs-network
    command: >
      sh -c "
        echo 'Waiting for services to be ready...' &&
        sleep 30 &&
        echo 'Pulling required Ollama models...' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"qwen3-embedding:0.6b\"}' &&
        curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"phi3:mini\"}' &&
        echo 'Installing spaCy model...' &&
        python -m spacy download en_core_web_sm &&
        echo 'Starting FastAPI server...' &&
        python main.py --step 5
      "

volumes:
  ollama_data:
    driver: local
  chromadb_data:
    driver: local
  pipeline_logs:
    driver: local

networks:
  hilabs-network:
    driver: bridge
